<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AURIGA Scrub Nurse ‚Äî David Genotelle</title>
  
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Syne:wght@600;700;800&family=Cabinet+Grotesk:wght@700;800;900&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />

  <link rel="stylesheet" href="css/style.css">
</head>
<body>

  <nav class="navbar">
    <div class="brand">
      <div class="logo-box">DG</div>
      <span class="logo-text">David Genotelle</span>
    </div>
    <a href="index.html" class="back-btn">
      <i class="fa-solid fa-arrow-left"></i> Back to Portfolio
    </a>
  </nav>

  <section class="page-hero">
    <div class="hero-eyebrow">
      <i class="fa-solid fa-robot"></i>
      Master's Research Project
    </div>
    <h1>AURIGA: Robotic <br/><span class="highlight">Scrub Nurse</span></h1>
    <p>Investigating the limitations of state-of-the-art imitation learning in medical robotics, to pave the way for safer, more flexible clinical assistants in the Operating Room.</p>
    <div class="hero-tags">
      <span class="hero-tag">ü§ñ Bimanual Robotics</span>
      <span class="hero-tag">üß† Imitation Learning</span>
      <span class="hero-tag">üî¨ Research & Evaluation</span>
      <span class="hero-tag">‚öôÔ∏è ROS2</span>
      <span class="hero-tag">üè• HealthTech</span>
    </div>
  </section>

  <div class="projects-section">

    <div class="project-detail-card reveal">
      <div class="card-inner">
        <div class="card-visual">
          <div class="auto-slider">
            <img src="assets/images/AURIGA/scrub_nurse_scene.webp" alt="Clinical Scrub Nurse Context">
          </div>
        </div>

        <div class="card-content theme-indigo">
          <div class="card-number">01</div>
          <div class="card-eyebrow">
            <span class="eyebrow-dot"></span>
            Clinical Motivation
          </div>
          <h2>Addressing the Healthcare Shortage</h2>
          <p>The medical field is facing an expected shortage of 2.3 million trained health workers in Europe by 2030. To mitigate this, robotic assistants must be developed to handle repetitive and risky tasks in the Operating Room.</p>
          <div class="highlights-mini">
            <div class="highlight-item">
              <i class="fa-solid fa-user-nurse" style="color:#6366f1"></i>
              <strong>Occupational Risks:</strong> Scrub nurses face high-stress environments involving repetitive motions, radiation exposure, and cutting hazards.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-robot" style="color:#6366f1"></i>
              <strong>Beyond Rule-Based Systems:</strong> Traditional robots (like Moxi) lack the flexibility needed to adapt to dynamic surgical procedures without constant expert reprogramming.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-brain" style="color:#6366f1"></i>
              <strong>The AI Need:</strong> Imitation Learning offers a pathway to train robots directly from human demonstrations, granting flexibility for complex medical tasks.
            </div>
          </div>
          <div class="card-tags">
            <span class="card-tag">Clinical Context</span>
            <span class="card-tag">Staff Shortage</span>
            <span class="card-tag">OR Dynamics</span>
          </div>
        </div>
      </div>
    </div>

    <div class="project-detail-card reveal delay-1">
      <div class="card-inner reverse">
        <div class="card-visual">
          <div class="auto-slider">
            <img src="assets/images/AURIGA/dobot_X-Trainer_transparent.png" alt="Dobot Setup 1">
            <img src="assets/images/AURIGA/demo_pick_syringe.jpeg" alt="Telemanipulation">
          </div>
        </div>

        <div class="card-content theme-teal">
          <div class="card-number">02</div>
          <div class="card-eyebrow">
            <span class="eyebrow-dot"></span>
            Hardware & Protocol
          </div>
          <h2>Bimanual Data Collection Platform</h2>
          <p>To train advanced AI policies, I set up a leader-follower telemanipulation architecture using the Dobot X-Trainer to gather multimodal, high-quality human demonstrations from clinical experts.</p>
          <div class="highlights-mini">
            <div class="highlight-item">
              <i class="fa-solid fa-code-branch" style="color:#14b8a6"></i>
              Configured a 14 Degrees of Freedom (DoF) bimanual platform.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-video" style="color:#14b8a6"></i>
              Collected synchronous multimodal data: Joint angles and 3x RGB-Depth camera feeds.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-clipboard-list" style="color:#14b8a6"></i>
              Designed tasks (sterile pack opening, instrument handover) to extract realistic clinical trajectories.
            </div>
          </div>
          <div class="card-tags">
            <span class="card-tag">Dobot X-Trainer</span>
            <span class="card-tag">Telemanipulation</span>
            <span class="card-tag">Multimodal Data</span>
          </div>
        </div>
      </div>
    </div>

    <div class="project-detail-card reveal delay-2">
      <div class="card-inner">
        <div class="card-visual">
          <div class="auto-slider">
            <img src="assets/images/AURIGA/inference_pick_syringe.jpeg" alt="ACT Spatial Generalization Evaluation">
          </div>
        </div>

        <div class="card-content theme-blue">
          <div class="card-number">03</div>
          <div class="card-eyebrow">
            <span class="eyebrow-dot"></span>
            Policy Testing & Limitations
          </div>
          <h2>Evaluating SoTA Policy Limits</h2>
          <p>Deployed the ALOHA ACT (Action Chunking with Transformers) policy and conducted a deep analysis of its execution capabilities, revealing critical limitations in how current models handle variance.</p>
          <div class="highlights-mini">
            <div class="highlight-item">
              <i class="fa-solid fa-ruler-combined" style="color:#2563eb"></i>
              <strong>Spatial Generalization:</strong> Success rates drop drastically when the initial pose of the object varies from the training data.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-lightbulb" style="color:#2563eb"></i>
              <strong>Environmental Sensitivity:</strong> The policy struggles to adapt to changing lighting conditions, shadows, and camera resolutions.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-chart-simple" style="color:#2563eb"></i>
              <strong>Metric Deficiencies:</strong> Highlighted that simply measuring "success rate" fails to capture how strictly the model mimics the demonstrated trajectory without contextual task understanding.
            </div>
          </div>
          <div class="card-tags">
            <span class="card-tag">ALOHA ACT</span>
            <span class="card-tag">Generalization limits</span>
            <span class="card-tag">SoTA Analysis</span>
          </div>
        </div>
      </div>
    </div>

    <!--
    <div class="project-detail-card reveal delay-2">
      <div class="card-inner">
        <div class="card-visual">
          <div class="auto-slider">
            <img src="assets/images/auriga_architecture_concept.jpg" alt="Research Architecture Concept">
          </div>
        </div>

        <div class="card-content theme-blue">
          <div class="card-number">03</div>
          <div class="card-eyebrow">
            <span class="eyebrow-dot"></span>
            Ongoing Research
          </div>
          <h2>Designing a Validation-Aware Architecture</h2>
          <p>Current policies act as "black boxes" with no intrinsic knowledge of whether a task was actually successful. I am currently designing a novel architecture to introduce explicit task validation and error recovery.</p>
          <div class="highlights-mini">
            <div class="highlight-item">
              <i class="fa-solid fa-sitemap" style="color:#2563eb"></i>
              <strong>Task Decomposition:</strong> Researching language-guided models (LLMs) to break down complex tasks into manageable sub-tasks.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-eye" style="color:#2563eb"></i>
              <strong>Explicit Validation:</strong> Exploring Visual Question Answering (VQA) to visually confirm if a sub-task is successfully completed.
            </div>
            <div class="highlight-item">
              <i class="fa-solid fa-rotate-left" style="color:#2563eb"></i>
              <strong>Error Recovery:</strong> Aiming to replace blind task execution with closed-loop supervision that allows the robot to retry upon failure.
            </div>
          </div>
          <div class="card-tags">
            <span class="card-tag">Research Phase</span>
            <span class="card-tag">VQA Validation</span>
            <span class="card-tag">Closed-loop</span>
          </div>
        </div>
      </div>
    </div> -->

  </div>

  <div class="footer-cta">
    <p>Want to see more of my background or discuss my research?</p>
    <a href="index.html" class="cta-btn">
      <i class="fa-solid fa-arrow-left"></i>
      Back to Portfolio
    </a>
  </div>

  <script>
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(e => {
        if (e.isIntersecting) {
          e.target.classList.add('active');
          observer.unobserve(e.target);
        }
      });
    }, { threshold: 0.12 });

    document.querySelectorAll('.reveal').forEach(el => observer.observe(el));

    // Automatically runs all sliders on the page
    document.querySelectorAll('.auto-slider').forEach(slider => {
      const mediaElements = slider.querySelectorAll('img, video');
      
      // If there is only 1 image, just show it and don't run the interval
      if (mediaElements.length <= 1) {
          if (mediaElements.length === 1) {
              mediaElements.classList.add('active'); // ADDED HERE
          }
          return; 
      }
      
      let currentIndex = 0;
      mediaElements[currentIndex].classList.add('active'); // ADDED HERE

      setInterval(() => {
        mediaElements[currentIndex].classList.remove('active'); 
        currentIndex = (currentIndex + 1) % mediaElements.length; 
        mediaElements[currentIndex].classList.add('active'); 
      }, 4000); 
    });
  </script>
</body>
</html>